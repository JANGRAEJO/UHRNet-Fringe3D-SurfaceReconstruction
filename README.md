# ðŸŒŠ UHRNet for Fringe Pattern to Height Map Conversion

UHRNet for Fringe Pattern to Height Map Conversion

This repository contains a PyTorch implementation and full training/inference pipeline for converting **structured-light fringe pattern images** into **surface height maps** using a deep learning model based on the UHRNet architecture.

Fringe pattern imagesâ€”generated through **structured light projection techniques**â€”encode 3D surface geometry by projecting sinusoidal patterns onto an object. As the pattern deforms over the object's shape, it carries depth-related information that can be interpreted to reconstruct a 3D profile.

This approach is particularly useful in **optical metrology** and **non-contact surface inspection**, where traditional methods such as phase unwrapping or multi-pattern acquisition can be complex or computationally expensive.

Our model provides a direct, end-to-end solution for **interpreting single fringe images** and reconstructing corresponding 3D height maps, enabling fast and scalable inspection pipelines in **precision manufacturing**, **semiconductor metrology**, and **optical surface analysis**.

## ðŸ“Œ Project Summary

This project aims to train a deep neural network to interpret interference fringe patterns and output corresponding surface height maps, enabling **non-contact, high-resolution surface measurement** at micrometer-level precision. This is a crucial step for building practical tools in optics and precision manufacturing systems.

## ðŸ“ Dataset

The original dataset used in this project is sourced from the supplementary materials of the paper:

- **Title**: [A deep-learning method for fringe pattern analysis with UHRNet](https://doi.org/10.1364/OE.485239)
- **Authors**: Huang et al., Optics Express, 2023
- **Original GitHub**: [https://github.com/fead1/UHRNet](https://github.com/fead1/UHRNet)

The dataset includes:
- `f1_80.npy`: Single fringe pattern input
- `Z.npy`: Corresponding ground truth height map

> ðŸ“Œ The original authors' pretrained model is not accessible outside China. Therefore, I trained the UHRNet model from scratch using their provided data.

## ðŸ§  Model & Training

### Architecture
- Model: UHRNet (modified version from the original GitHub)
- Framework: PyTorch
- Loss Function: MSE (Mean Squared Error)
- Training: Full training from scratch using **all available data** (not just 128 samples as used in the original example)

### My Improvements:
- âœ… Full-dataset training instead of partial
- âœ… Batched prediction and inference support
- âœ… Visualization code to compare:
  - Input Fringe Pattern
  - Ground Truth Height Map
  - Predicted Height Map
- âœ… Optional 3D surface plot generation using `matplotlib`

## ðŸŽ¯ Purpose

This work serves as the foundation for an optical measurement system I'm developing that:
- Uses fringe patterns captured by lenses (e.g., Michelson Interferometer)
- Applies deep learning to **interpret optical data without complex phase-unwrapping or classical algorithms**
- Enables **precise surface flatness inspection** in real time

### ðŸ—† Understanding the Demo Output

The visualization below shows three side-by-side images for each sample:

| Input | Ground Truth | Prediction |
|:--:|:--:|:--:|
| !(output_vis/vis_00.png)|

- **Input Fringe Pattern**: The grayscale structured-light image captured from a projector-camera system. It contains subtle intensity changes based on the objectâ€™s surface shape.
- **Ground Truth Height Map**: The real 3D shape (height in mm or scaled units) of the object, used for supervision during training. This is usually obtained using an accurate 3D scanning method.
- **Predicted Height Map**: The output from our UHRNet model. It attempts to reconstruct the objectâ€™s depth based only on the single fringe image.

> Color maps: Red/Yellow = High surface, Blue = Low surface.

---

### ðŸŽ® Demo GIF

![Visualization Demo](output_vis/visualization_result.gif)

## ðŸ“Š Inference & Output

- Inference script: `visualize_prediction.py`
- Output:
  - `.npy` files for predictions and ground truth
  - PNG visualizations (side-by-side fringe, ground truth, predicted)
  - Optional MP4 video made from results
  - Optional 3D surface plots for more intuitive interpretation

## ðŸ§± Directory Structure

```
UHRNet/
â”‚
â”œâ”€â”€ data_npy/               # Fringe and height map input
â”‚   â”œâ”€â”€ f1_80.npy
â”‚   â””â”€â”€ Z.npy
â”‚
â”œâ”€â”€ output_vis/             # Output images and .npy predictions
â”‚   â”œâ”€â”€ vis_0000.png
â”‚   â”œâ”€â”€ preds.npy
â”‚   â””â”€â”€ gt.npy
â”‚
â”œâ”€â”€ visualize_prediction.py # Main inference & visualization script
â”œâ”€â”€ make_video_from_images.py
â”œâ”€â”€ make_3d_plot.py         # Optional 3D surface rendering (if enabled)
â””â”€â”€ uhrnet_architecture.py  # UHRNet model definition
```

## ðŸ“Š Model Performance

The model was trained on the full [SIDO 3D Reconstruction Dataset](https://figshare.com/articles/dataset/Single-input_dual-output_3D_shape_reconstruction/19709134).  
Unlike the original paper which used only 128 samples, this implementation utilizes the entire dataset for more robust generalization.

| Metric                  | Value         |
|------------------------|---------------|
| **Validation MSE**     | 13.1037       |
| **Validation SSIM**    | 0.8698        |
| **Test Input Resolution** | 640 Ã— 352 |
| **Model Size**         | 30.33M params |

> ðŸ“Œ The original paper reported a mean RMSE of 0.443mm and SSIM of 0.9978, but they used a smaller test set (only 128 images).  
> My results are on the full dataset, so direct comparison should consider this difference.


## ðŸ”­ Future Work

- [ ] Integrate with real-time camera feed and depth prediction (MiDaS or self-trained stereo)
- [ ] Improve model precision via data augmentation or simulated fringe patterns
- [ ] Explore transformer-based models for fringe interpretation
- [ ] Apply to **multi-fringe** or **phase-shifting** setups
- [ ] Add GUI for real-time inspection and measurement

---

## ðŸ‘¤ Author

**Jangrae (William) Jo**  
M.S. in Electrical and Computer Engineering @ UMass Amherst  
 
Optical Metrology | Computer Vision | Deep Learning  

---

## ðŸ“œ Citation

If you use this repository, please cite the original paper by Huang et al. (2023) and link this repository for credit to my PyTorch training adaptation.


## ðŸ› ï¸ Full Installation & Execution Guide

### ðŸ“¦ 1. Environment Setup
```bash
# (Optional) Create and activate a virtual environment
conda create -n fringe_env python=3.9 -y
conda activate fringe_env

# Install required dependencies
pip install -r requirements.txt
```

---

### ðŸ“ 2. Download the Dataset

Download the SIDO dataset:

- Paper: https://www.sciencedirect.com/science/article/abs/pii/S0030428422003607  
- Dataset: https://figshare.com/articles/dataset/Single-input_dual-output_3D_shape_reconstruction/19709134

Place the `.npy` files in a `data_npy/` directory:

```
project_root/
â”œâ”€â”€ data_npy/
â”‚   â”œâ”€â”€ f1_80.npy
â”‚   â””â”€â”€ Z.npy
```

---

### ðŸ§  3. Train the Model
```bash
python train.py
```

- The trained model weights will be saved as `UHRNet_weight.pth`.

---

### ðŸ“ˆ 4. Run Inference & Visualize Results
```bash
python visualize_prediction.py
```

- This will save predicted images and `.npy` files in the `output_vis/` directory.

---

### ðŸŽžï¸ 5. Create Video & GIF from Predictions

To generate an MP4 video:
```bash
python make_video_from_images.py
```

To generate a GIF:
```python
from moviepy.editor import ImageSequenceClip

clip = ImageSequenceClip("output_vis", fps=1)
clip.write_gif("output_vis.gif")
```

---

### ðŸ“„ Example `requirements.txt`
```text
numpy
torch>=1.12
matplotlib
opencv-python
moviepy
```
